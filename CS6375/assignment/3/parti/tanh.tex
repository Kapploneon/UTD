\documentclass[preview]{standalone}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage{hyperref}
\usepackage{cleveref}

\begin{document}

\section{}

\subsection{Training Rule for Output Unit Weights}
For output unit, based on textbook:

\begin{align}
\label{o0}
\frac{\partial E_d}{\partial net_j}=\frac{\partial E_d}{\partial o_j}\frac{\partial o_j}{\partial net_j}
\end{align}

Consider the first term:

\begin{align}
\label{o1}
\begin{split}
\frac{\partial E_d}{\partial o_j}
&=\frac{\partial}{\partial o_j}\frac{1}{2}\sum_d(t_d-o_d)^2\qquad\texttt{//only $d = j$ matters}\\
&=\frac{\partial}{\partial o_j}\frac{1}{2}(t_j-o_j)^2\\
&=\frac{1}{2} \cdot 2 \cdot (t_j-o_j)\frac{\partial(t_j-o_j)}{\partial o_j}\\
&=-(t_j-o_j)
\end{split}
\end{align}

Consider the second term, since the \texttt{identity} function is used, the
derivative is just the derivative of the \texttt{identity} function, which is
$1$.

\begin{align}
\label{o2}
\begin{split}
\frac{\partial o_j}{\partial net_j}=\frac{\partial}{\partial net_j}\mathtt{identity}(net_j) = 1
\end{split}
\end{align}

Substituting \cref{o1} and \cref{o2} into \cref{o0}:

\begin{align}
    \frac{\partial E_d}{\partial net_j}=-(t_j-o_j)
\end{align}

Thus, the gradient descent rule for output units is:

\begin{align}
\Delta w_{ji} = -\eta\frac{\partial E_d}{\partial w_{ji}} = \eta \cdot
(t_j-o_j) \cdot x_{ji}
\end{align}


\subsection{Training Rule for Hidden Unit Weights}
For hidden unit, first calculate the derivative of $\tanh()$.

\begin{align}
\label{tanhderiv}
\begin{split}
\frac{\partial}{\partial x}\tanh(x)
&=\frac{\partial}{\partial x}\frac{\sinh(x)}{\cosh(x)}\\
&=\cfrac{\cfrac{\partial}{\partial
    x}\sinh(x)\times\cosh(x)-\cfrac{\partial}{\partial
    x}\cosh(x)\times\sinh(x)}{\cosh^2(x)}\\
&=\frac{\cosh^2(x)-\sinh^2(x)}{\cosh^2(x)}\\
&=1-\frac{\sinh^2(x)}{\cosh^2(x)}\\
&=1-\tanh^2(x)
\end{split}
\end{align}

Based on textbook:

\begin{align}
\label{i0}
\begin{split}
\frac{\partial E_d}{\partial net_j}
&=\sum_{{k}\in{Downstream(j)}}\frac{\partial E_d}{\partial net_{{k}}}\frac{\partial net_{{k}}}{\partial net_{j}}\\
&=\sum_{{k}\in{Downstream(j)}}-\delta_{{k}}\frac{\partial net_{{k}}}{\partial net_{j}}
\end{split}
\end{align}

%For the first term in \cref{i0}, applying the chain rule again as:
%\begin{align}
%\label{i1}
%\frac{\partial E_d}{\partial net_{{k}}}=\frac{\partial E_d}{\partial
%o_{{k}}}\frac{\partial o_{{k}}}{\partial net_{{k}}}
%\end{align}

%We have already calculated the first part of \cref{i1} as
%$\displaystyle\frac{\partial E_d}{\partial o_{{k}}}=-(t_{{k}}-o_{{k}})$. And
%for the second part, since the $\tanh()$ function is used, the derivative is
%just the derivative of the $\tanh()$ function, which is \cref{tanhderiv}. Thus,
%we have:

%\begin{align}
%\label{i2}
%\frac{\partial E_d}{\partial net_{{k}}}=-(t_{{k}}-o_{{k}})\cdot(1-o_{{k}}^2)
%\end{align}

For the second term in \cref{i0}:

\begin{align}
\label{i3}
\begin{split}
\frac{\partial net_{{k}}}{\partial net_{j}}
&= \frac{\partial net_{{k}}}{\partial o_j}\frac{\partial o_j}{\partial net_{j}}\\
&= w_{{kj}}\cdot\frac{\partial o_j}{\partial net_{j}}\\
&= w_{{kj}}\cdot(1-o_j^2)\quad\texttt{// apply \cref{tanhderiv} again}
\end{split}
\end{align}

Substitute into \cref{i0}
\begin{align}
\label{i4}
\frac{\partial E_d}{\partial net_{{k}}}
&= \sum_{{k\in Downstream(j)}}-\delta_{{k}}\cdot w_{{kj}} \cdot(1-o_j^2)
\end{align}

Rearranging terms and using $\delta_j$ to denote $-\frac{\partial E_d}{\partial
net_j}$, we have:

\begin{align}
    \delta_j = -(1-o_j^2)\sum_{{k\in Downstream(j)}}\delta_{{k}}\cdot w_{{kj}}
\end{align}

and the gradient descent rule for hidden units:

\begin{align}
    \Delta w_{ji} = \eta \cdot \delta_j \cdot x_{ji}
\end{align}

\qed
\end{document}
