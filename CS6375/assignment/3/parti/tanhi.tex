\documentclass[preview]{standalone}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage{hyperref}
\usepackage{cleveref}

\begin{document}

For hidden unit, first calculate the derivative of $\tanh()$.

\begin{align}
\begin{split}
\frac{\partial}{\partial x}\tanh(x)
&=\frac{\partial}{\partial x}\frac{\sinh(x)}{\cosh(x)}\\
&=\cfrac{\cfrac{\partial}{\partial
    x}\sinh(x)\times\cosh(x)-\cfrac{\partial}{\partial
    x}\cosh(x)\times\sinh(x)}{\cosh^2(x)}\\
&=\frac{\cosh^2(x)-\sinh^2(x)}{\cosh^2(x)}\\
&=1-\frac{\sinh^2(x)}{\cosh^2(x)}\\
&=1-\tanh^2(x)
\end{split}
\end{align}

Based on textbook:

\begin{align}
\label{i0}
\begin{split}
\frac{\partial E_d}{\partial net_j}
&=\sum_{{k}\in{Downstream(j)}}\frac{\partial E_d}{\partial net_{{k}}}\frac{\partial net_{{k}}}{\partial net_{j}}\\
\end{split}
\end{align}

For the first term, applying the chain rule again as:
\begin{align}
\label{o0}
\frac{\partial E_d}{\partial net_j}=\frac{\partial E_d}{\partial o_j}\frac{\partial o_j}{\partial net_j}
\end{align}

\begin{align}
\label{o1}
\begin{split}
\frac{\partial E_d}{\partial o_j}
&=\frac{\partial}{\partial o_j}\frac{1}{2}\sum_d(t_d-o_d)^2\qquad\texttt{//only $d = j$ matters}\\
&=\frac{\partial}{\partial o_j}\frac{1}{2}(t_j-o_j)^2\\
&=\frac{1}{2} \cdot 2 \cdot (t_j-o_j)\frac{\partial(t_j-o_j)}{\partial o_j}\\
&=-(t_j-o_j)
\end{split}
\end{align}

Consider the second term, since the \texttt{identity} function is used, the
derivative is just the derivative of the \texttt{identity} function, which is
$1$.

\begin{align}
\label{o2}
\begin{split}
\frac{\partial o_j}{\partial net_j}=\frac{\partial}{\partial net_j}\mathtt{identity}(net_j) = 1
\end{split}
\end{align}


\begin{align}
    \frac{\partial E_d}{\partial net_j}=-(t_j-o_j)
\end{align}

Thus, the gradient descent rule for output units is:

\begin{align}
\Delta w_{ji} = -\eta\frac{\partial E_d}{\partial w_{ji}} = \eta \cdot (t_j-o_j)
\end{align}

\end{document}
