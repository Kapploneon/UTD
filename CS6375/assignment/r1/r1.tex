\documentclass[10pt]{article}

\usepackage[l2tabu, orthodox]{nag}
\usepackage{tabu}
\usepackage[onehalfspacing]{setspace}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

\usepackage{cite}

\newtheorem{theorem}{Theorem}%[section]

\usepackage[prefix=tikzsym]{tikzsymbols}

% As a rule of thumb it should be loaded at the end of the preamble, after all
% the other packages. A few exceptions exist, such as the cleveref package that
% is also mentioned in this post. Hence, cleveref should be loaded after
% hyperref.
\usepackage{hyperref}

% This package introduces the \cref command. When using this command to make
% cross-references, instead of \ref or \eqref, a word is placed in front of the
% reference according to the type of reference: fig. for figures, eq. for
% equations
\usepackage{cleveref}

% Setup hyperref package, and colours for links
\hypersetup{
%    bookmarks=true,         % show bookmarks bar?
    %unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={Assignment},    % title
    pdfauthor={Hanlin He},     % author
    %pdfsubject={Subject},   % subject of the document
    pdfcreator={Hanlin He},   % creator of the document
    %pdfproducer={Producer}, % producer of the document
    %pdfkeywords={Algorithm, Homework}, % list of keywords
    pdfnewwindow=true,      % links in new PDF window
    colorlinks=false,       % false: boxed links; true: colored links
    %linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    %linkbordercolor={1 0 0},
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan,           % color of external links
    plainpages=false,
    pageanchor=false,       %put an anchor on every page
}

\title{Reading Assignment 1}
\author{Hanlin He (hxh160630)}
\date{\today}

\begin{document}
\maketitle

Questions from paper by Pedro Domingos
\cite{Domingos:2012:FUT:2347736.2347755}.

\section{Introduction}

\subsection{What is the definition of ML?}

Machine learning enables systems automatically learn programs from data.
And according to Tom Mitchell, machine learning is:

\begin{quote}
    A computer program is said to learn from \emph{experience E} with respect
    to some \emph{task T} and some \emph{performance measure P}, if its
    performance on T, as measured by P, \textbf{improves} with experience E.
\end{quote}

\subsection{What is a classifier?}

A \emph{classifier} is a system that inputs (typically) a vector of discrete
and/or continuous \emph{feature values} and outputs a single discrete value,
the \emph{class}.

\section{Learning}

\subsection{What are the 3 components of a learning system?}

Three components are:

\begin{itemize}
    \item \textbf{Representation.} A classifier must be represented in some
        formal language that the computer can handle. Conversely, choosing a
        representation for a learner is tantamount to choosing the set of
        classifiers that it can possibly learn.

    \item \textbf{Evaluation.} An evaluation function (also called
        \emph{objective function} or \emph{scoring function}) is needed to
        distinguish good classifiers from bad ones. The evaluation function
        used internally by the algorithm may differ from the external one that
        we want the classifier to optimize, for ease of optimization (see
        below) and due to the issues discussed in the next section.

    \item \textbf{Optimization.} We need to search among the classifiers in the
        language for the highest-scoring one, i.e., optimizing the model to
        predict better.

\end{itemize}

\subsection{Define Information Gain}

In machine learning, \textbf{Information Gain} is the same as \emph{mutual
information}, the expected value of the Kullback–Leibler divergence of the
univariate probability distribution of one variable from the conditional
distribution of this variable given the other one.

In general terms, the expected information gain is the change in
\emph{information entropy} $H$ from a prior state to a state that takes some
information as given:

\begin{equation}
    IG(T,a)=H(T) - H(T | a)
\end{equation}

Formally, let $T$ denote a set of training examples, each of the form
$(\mathtt{x},y)=(x_1,x_2,x_3,\ldots,x_k,y)$ where $x_a \in vals(a)$ is the
value of the $a$th attribute of example $\mathtt{x}$ and $y$ is the
corresponding class label. The information gain for an attribute $a$ is defined
in terms of entropy $H$ as follow:

\begin{equation}
    IG(T,a)=H(T)-\sum_{v \in vals(a)} \frac{|\{\mathtt{x}\in T |
    x_a=v\}|}{|T|}\cdot H\left(\left\{\mathtt{x}\in T | x_a=v\right\}\right)
\end{equation}

The mutual information is equal to the total entropy for an attribute if for
each of the attribute values a unique classification can be made for the result
attribute. In this case, the relative entropies subtracted from the total
entropy are 0.

\section{Generalization}

\subsection{Why is generalization more important?}

Since no matter how much data we have, it is very unlikely that we will see
those exact examples again at test time.

\subsection{What is cross-validation? }

\textbf{Cross-validation} is randomly dividing the training data into several
subsets, holding out each one while training on the rest, testing each learned
classifier on the examples it did not see, and averaging the results to see how
well the particular parameter setting does.

A big advantage of cross-validation is keep the data size for training, since
every part of the data would be used to for training.

\subsection{How is generalization different from other?}

Unlike in most other optimization problems, in generalization we don’t have
access to the function we want to optimize. We have to use training error as a
surrogate for test error, and this is fraught with danger.

\section{Data Alone is Not Enough}

\subsection{Scenarios}

With 10 boolean variables, there are $2^{10} = 1024$ possible examples. If we
have seen 100 examples, we approximately have $10\%$ of all instance space.

\subsection{What is the ``No Free Lunch'' Theorem?}

The NFL theorem first hypothesizes that objective functions do not change while
optimization is in progress, and then hypothesizes that objective functions may
change.

\begin{theorem}
    For any algorithms $a_1$ and $a_2$, at iteration step $m$,
    \[\sum_f P\left(d_m^y|f,m,a_1\right)=\sum_f P(d_m^y|f,m,a_2)\text{,}\]
    where $d_m^y$ denotes the ordered set of size $m$ of the cost values $y$
    associated to input values $x\in X$, $f:X \rightarrow Y$ is the function
    being optimized and $P(d_m^y|f,m,a)$ is the conditional probability of
    obtaining a given sequence of cost values from algorithm $a$ run $m$ times
    on function $f$.
\end{theorem}

\subsection{General Assumptions and Meaning of Induction}

\paragraph{The general assumption we are using is:} \emph{similar examples
often having similar classes, limited dependences, or limited complexity}. This
assumption is a large part of why machine learning has been so successful.

\paragraph{As for Induction,} in general, induction turns a small amount of
input knowledge into a large amount of output knowledge. It is very useful
since induction requires much less input knowledge to produce useful results.
Although induction still needs more than zero input knowledge to work, as with
any lever, the more we put in, the more we can get out.

\subsection{How is learning like farming? \tikzsymbolsuse{Smiley}}

Just like farming, learning lets nature do most of the work, rather than
engineer doing a lot of work.

\section{Overfitting}

\subsection{What is Overfitting?}

If the data is not sufficient to completely determine the correct classifier,
then the classifier we computed would not be gounded in reality. This problem
is called \emph{overfitting}

Sometimes the learner outputs a classifier that is $100\%$ accurate on the
training data but only $50\%$ accurate on test data. If we only look on the
performance on training data, we might think the classifier works perfectly.

\subsection{What is Meant by Bias and Variance?}

\emph{Bias} is a learner’s tendency to consistently learn the same wrong thing.
\emph{Variance} is the tendency to learn random things irrespective of the real
signal.

\subsection{What can help us combat overfitting?}

Cross-validation can help to combat overfitting, for example by using it to
choose the best size of decision tree to learn.

One popular method is adding a \emph{regularization term} to the evaluation
function. This can, for example, penalize classifiers with more structure,
thereby favoring smaller ones with less room to overfit.

Another option is to perform a statistical significance test like chi-square
before adding new structure, to decide whether the distribution of the class
really is different with and without this structure.

\section{Intuition Fails in High Dimensions}

\subsection{Why do algorithms that work well in lower dimensions fail at higher
dimensions?}

This is because generalizing correctly becomes \emph{exponentially harder} as
the dimensionality (number of features) of the examples grows, because a
fixed-size training set covers a dwindling fraction of the input space.

And our intuitions, which come from a three-dimensional world, often do not
apply in high-dimensional ones.

\subsection{What is meant by ``blessing of non-uniformity''?}

In most applications examples are not spread uniformly throughout the instance
space, but are concentrated on or near a lower-dimensional manifold.

For example, $k$-nearest neighbor works quite well for handwritten digit
recognition even though images of digits have one dimension per pixel, because
the space of digit images is much smaller than the space of all possible
images. Learners can implicitly take advantage of this lower effective
dimension, or algorithms for explicitly reducing the dimensionality can be
used.

\section{Theoretical Guarantees}

\subsection*{What has been one of the major developments in the recent decades
about results of induction?}

The major development is the realization that in fact we can have guarantees on
the results of induction, particularly if we’re willing to settle for
probabilistic guarantees.

\section{Feature Engineering}

\subsection{What is the most important factor that determines whether a machine
learning project succeeds?}

The most important factor is the features used in the machine learning project.

If the data have many independent features that each correlate well with the
class, learning is easy. On the other hand, if the class is a very complex
function of the features, you may not be able to learn it.

\subsection{In a ML project, which is more time consuming – feature engineering
or the actual learning process? Explain how ML is an iterative process?}

Feature engineering is much more time consuming in a machine learning project.
Feature engineering is more difficult because it’s domain-specific, while
learners can be largely general-purpose.

Machine learning is an iterative process of running the learner, analyzing the
results, modifying the data and/or the learner, and repeating.

\subsection{What, according to the author, is one of the holy grails of ML?}

Quote from the paper:

\begin{quote}
    To automate more and more of the feature engineering process.
\end{quote}

One way this is often done today is by automatically generating large numbers
of candidate features and selecting the best by (say) their information gain
with respect to the class.

\section{More Data Beats A Cleverer Algorithm}

\subsection{If your ML solution is not performing well, what are two things
that you can do? Which one is a better option?}

There are two main choices:

\begin{enumerate}
    \item design a better learning algorithm,
    \item gather more data (more examples, and possibly more raw features,
        subject to the curse of dimensionality).
\end{enumerate}

The second one is the better option. As a rule of thumb, a dumb algorithm with
lots and lots of data beats a clever one with modest amounts of it.

\subsection{What are the 3 limited resources in ML computations? What is the
bottleneck today? What is one of the solutions?}

3 limited resources in machine learning computations are \emph{time},
\emph{memory} and \emph{training data}. Which one is the bottleneck has changed
from decade to decade. Today, it is time, since enormous mountains of data are
available, but there is not enough time to process it, so it goes unused.

A solution is to come up with fast ways to learn complex classifiers, and
indeed there has been remarkable progress in this direction

\subsection{A surprising fact mentioned by the author is that all
representations (types of learners) essentially ``all do the same''. Can you
explain? Which learners should you try first?}

Because all learners essentially work by grouping nearby examples into the
same class. The key difference is in the meaning of ``nearby''. With
non-uniformly distributed data, learners can produce widely different frontiers
while still making the same predictions in the regions that matter (those with
a substantial number of training examples, and therefore also where most test
examples are likely to appear).

As a rule, it pays to try the simplest learners first (e.g., naive Bayes before
logistic regression, k-nearest neighbor before support vector machines). More
sophisticated learners are seductive, but they are usually harder to use,
because they have more knobs you need to turn to get good results, and because
their internals are more opaque.

\subsection{The author divides learners into two types based on their
representation size. Write a brief summary.}

Learners can be divided into two major types: those \emph{whose representation
has a fixed size}, like linear classifiers, and those \emph{whose
representation can grow with the data}, like decision trees.

Fixed-size learners can only take advantage of so much data. Variable-size
learners can in principle learn any function given sufficient data, but in
practice they may not, because of limitations of the algorithm (e.g., greedy
search falls into local optima) or computational cost.

\section{Learn many models, not just one}

\subsection*{Is it better to have variation of a single model or a combination
of different models, known as ensemble or stacking? Explain briefly.}

\paragraph{Creating such model ensembles is now standard.} In the simplest
technique, called bagging, we simply generate random variations of the training
set by resampling, learn a classifier on each, and combine the results by
voting. This works because it greatly reduces variance while only slightly
increasing bias.

\section{Simplicity does not imply accuracy}

\subsection{Read the last paragraph and explain why it makes sense to prefer
simpler algorithms and hypotheses.}

Because simplicity is a virtue in its own right, not because of a hypothetical connection with accuracy.

\section{Representable does not imply learnable}

\section{Correlation does not imply causation}

\subsection{It has been established that correlation between independent
variables and predicted variables does not imply causation, still correlation
is used by many researchers. Explain briefly the reason.}

Machine learning is usually applied to \emph{observational} data, where the
predictive variables are not under the control of the learner, as opposed to
\emph{experimental} data, where they are. Some learning algorithms can
potentially extract causal information from observational data, but their
applicability is rather restricted. On the other hand, correlation is a sign of
a potential causal connection, and we can use it as a guide to further
investigation.

\bibliography{r1bib}{}
\bibliographystyle{plain}

\end{document}
