\documentclass[12pt, letterpaper]{article}

\usepackage[l2tabu, orthodox]{nag}
\usepackage{tabu}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{graphicx}
\usepackage{color}
\usepackage{standalone}

\usepackage{parskip}
% As a rule of thumb it should be loaded at the end of the preamble, after all
% the other packages. A few exceptions exist, such as the cleveref package that
% is also mentioned in this post. Hence, cleveref should be loaded after
% hyperref.
\usepackage{hyperref}
\definecolor{linkcolour}{rgb}{0.8,0.2,0.5}
\hypersetup{colorlinks,breaklinks,urlcolor=linkcolour, linkcolor=linkcolour}

% This package introduces the \cref command. When using this command to make
% cross-references, instead of \ref or \eqref, a word is placed in front of the
% reference according to the type of reference: fig. for figures, eq. for
% equations
\usepackage{cleveref}

\title{Assignment 4 Part I}
\author{Hanlin He\footnote{hxh160630@utdallas.edu},
Tao Wang\footnote{txw162630@utdallas.edu}}

\begin{document}
\maketitle

\section{}

\subsection{}
\[P(w_h|g_a)=\frac{P(g_a|w_h) \times P(w_h)}{P(g_a)}=\frac{0.99 \times 0.85}{0.95}=0.8857\]

\subsection{}
\begin{align*}
\begin{split}
P(g_a|\lnot w_h)
&=\frac{P(\lnot w_h|g_a) \times P(g_a)}{P(\lnot w_h)}\\
&=\frac{\big(1-P(w_h|g_a)\big) \times P(g_a)}{1-P(w_h)}\\
&=\frac{\Big(1-\frac{P(g_a|w_h) \times P(w_h)}{P(g_a)}\Big) \times P(g_a)}{1-P(w_h)}\\
&=\frac{P(g_a)-P(g_a|w_h) \times P(w_h)}{1-P(w_h)}\\
% &=\cfrac{P(g_a)-P(w_h|g_a) \times P(g_a)}{1-P(w_h)}\\
% &=\cfrac{P(g_a)-P(w_h|g_a) \times P(g_a)}{1-P(w_h)}\\
&=\frac{0.95-0.99\times0.85}{1-0.85}=0.7233
\end{split}
\end{align*}

\section{}

Let $B$ denotes the taxi was blue, $SB$ denotes the taxi looked blue. Then:
\[P(SB|B)=0.75\]
and
\[P(SB|\lnot B)=0.25\]

We need to compute $P(B|SB)$.

Given the prior $P(B)=0.1$, we have:

\begin{align*}
    P(B|SB)&=\frac{P(SB|B)P(B)}{P(SB)}\\
           &=\frac{0.75\times0.1}{x}\\
           &=\frac{0.075}{x}\\
    P(\lnot B|SB)&=\frac{P(SB|\lnot B)P(\lnot B)}{P(SB)}\\
                 &=\frac{0.25\times0.9}{x}\\
                 &=\frac{0.225}{x}
\end{align*}
where $x$ is some constant.

Obviously, $\lnot B$ has greater probability. Thus, the taxi was more likely a
green taxi.

\pagebreak

\section{}

\subsection{}

Using naive bayes classifier, assuming all three factors are independent given mood.

\begin{align*}
P(No|Good, Pass, Out)
&=\frac{P(Good, Pass, Out|No){P(No)}}{P(Good, Pass, Out)}\\
&=\frac{P(Good|No) P(Pass|No) P(Out|No){P(No)}}{P(Good) P(Pass) P(Out)}\\
&=\frac{\frac{3}{5}\times\frac{1}{5}\times\frac{3}{5}\times\frac{5}{8}}{\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}}\\
&=\frac{\frac{9}{200}}{\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}}\\\\
P(Yes|Good, Pass, Out)
&=\frac{P(Good, Pass, Out|Yes){P(Yes)}}{P(Good, Pass, Out)}\\
&=\frac{P(Good|Yes) P(Pass|Yes) P(Out|Yes){P(Yes)}}{P(Good) P(Pass) P(Out)}\\
&=\frac{\frac{1}{3}\times\frac{3}{3}\times\frac{1}{3}\times\frac{3}{8}}{\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}}\\
&=\frac{\frac{3}{72}}{\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}}\\
\end{align*}

It's easy to compute $\arg\max = P(No|Good, Pass, Out)$. We can predict his
happiness is \emph{NO}.

\subsection{}

Using bayes classifier, we compute $\arg\max P(y|Good, Pass, Out)$ directly:

\begin{align*}
P(No|Good, Pass, Out)
&=\frac{P(Good, Pass, Out|No){P(No)}}{P(Good, Pass, Out)}\\
&=\frac{\frac{0}{5}\times\frac{5}{8}}{\frac{1}{8}}\\
P(Yes|Good, Pass, Out)
&=\frac{P(Good, Pass, Out|Yes){P(Yes)}}{P(Good, Pass, Out)}\\
&=\frac{\frac{1}{3}\times\frac{3}{8}}{\frac{1}{8}}
\end{align*}

It's easy to compute $\arg\max = P(Yes|Good, Pass, Out)$. We can predict his happiness is \emph{YES}.

\section{}

Let 
\begin{itemize}
\item $C$ denotes \emph{programmers can program in C++}
\item $J$ denotes \emph{programmers can program in Java}
\item $M$ denotes \emph{programmers is Macrosoft employee}.
\end{itemize}

\begin{align*}
P(C)&=0.5\\
P(J)&=0.4\\
P(M)&=0.01\\
P(C|M)&=0.99\\
P(J|M)&=0.98\\
&\Downarrow\text{Naive Bayes Assumption}\\
P(C \land J)&=P(C)P(J)=0.5\times0.4=0.2\\
P(C \land J | M)&=P(C | M)P(J | M)=0.99\times0.98=0.9702\\
&\Downarrow\text{Bayes Rule}\\
P(M|C\land J)&=\frac{P(C\land J | M)P(M)}{P(C \land J)}=\frac{0.9702\times0.01}{0.2}=0.04851
\end{align*}

\section{}

Using the notation in the hint. The equation (20) of the textbook can be
written as:

\begin{align*}
\sum_i\ln\frac{P(X_i|Y=0)}{P(X_i|Y=1)}
&=\sum_i\ln\frac{\theta_{i0}^{X_i}(1-\theta_{i0})^{(1-X_i)}}{\theta_{i1}^{X_i}(1-\theta_{i1})^{(1-X_i)}}\\
&=\sum_i\left(\ln\frac{\theta_{i0}^{X_i}}{\theta_{i1}^{X_i}}+\ln\frac{(1-\theta_{i0})^{(1-X_i)}}{(1-\theta_{i1})^{(1-X_i)}}\right)\\
&=\sum_i\Bigg(\ln\bigg(\frac{\theta_{i0}}{\theta_{i1}}\bigg)^{X_i}+\ln\bigg(\frac{1-\theta_{i0}}{1-\theta_{i1}}\bigg)^{(1-X_i)}\Bigg)\\
&=\sum_i\left(\ln\frac{\theta_{i0}}{\theta_{i1}}\cdot{X_i}+\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\cdot{(1-X_i)}\right)\\
&=\sum_i\Bigg(\bigg(\ln\frac{\theta_{i0}}{\theta_{i1}}-\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\bigg)\cdot{X_i}+\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\Bigg)
\end{align*}

Note this expression is a linear weighted sum of the $X_i$'s. Substituting
the expression back into equation (19) in textbook, we have

\[P(Y=1|X)=\cfrac{1}{1+\exp\bigg(\ln\frac{1-\pi}{\pi}+\sum_i\Big(\big(\ln\frac{\theta_{i0}}{\theta_{i1}}-\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\big)\cdot{X_i}+\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\Big)\bigg)}\]
Or equivalently,
\[P(Y=1|X)=\cfrac{1}{1+\exp(w_0+\sum_{i=1}^nw_iX_i)}\]
where the weights $w_1\ldots w_n$ are given by:
\[w_i=\ln\frac{\theta_{i0}}{\theta_{i1}}-\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\]
and where
\[w_0=\ln\frac{1-\pi}{\pi}+\sum_i\ln\frac{1-\theta_{i0}}{1-\theta_{i1}}\]
Also we have
\[P(Y=0|X)=1-P(Y=1|X)=\cfrac{\exp(w_0+\sum_{i=1}^nw_iX_i)}{1+\exp(w_0+\sum_{i=1}^nw_iX_i)}\]

\section{}

Substitute $P(X_i|Y=j)=h(X_i,\theta_{ij})\exp(-\theta_{ij}^TX_i+c)$ into the equation (20) of the textbook:
\begin{align*}
\sum_i\ln\frac{P(X_i|Y=0)}{P(X_i|Y=1)}
&=\sum_i\ln\frac{h(X_i,\theta_{i0})\exp(-\theta_{i0}^TX_i+c)}{h(X_i,\theta_{i1})\exp(-\theta_{i1}^TX_i+c)}\\
&=\sum_i\ln\frac{h(X_i,\theta_{i0})}{h(X_i,\theta_{i1})}+\ln\frac{\exp(-\theta_{i0}^TX_i+c)}{\exp(-\theta_{i1}^TX_i+c)}\\
&=\sum_i\ln\frac{h(X_i,\theta_{i0})}{h(X_i,\theta_{i1})}+\ln\exp\big({(-\theta_{i0}^TX_i+c)}-{(-\theta_{i1}^TX_i+c)}\big)\\
&=\sum_i\ln\frac{h(X_i,\theta_{i0})}{h(X_i,\theta_{i1})}+\big({\theta_{i1}^T-\theta_{i0}^T\big)\cdot X_i}\\
&=\sum_i\big({\theta_{i1}^T-\theta_{i0}^T\big)\cdot X_i}+\ln\frac{h(X_i,\theta_{i0})}{h(X_i,\theta_{i1})}\\
\end{align*}

Note this expression is again a linear weighted sum of the $X_i$'s. Substituting
the expression back into equation (19) in textbook, we have

\[P(Y=1|X)=\cfrac{1}{1+\exp\bigg(\ln\frac{1-\pi}{\pi}+\sum_i\Big(\big({\theta_{i1}^T-\theta_{i0}^T\big)\cdot X_i}+\ln\frac{h(X_i,\theta_{i0})}{h(X_i,\theta_{i1})}\Big)\bigg)}\]

Thus, we have:
\begin{align*}
w_0&=\ln\frac{1-\pi}{\pi}+\sum_i\ln\frac{h(X_i,\theta_{i0})}{h(X_i,\theta_{i1})}\\
w_i&=\theta_{i1}^T-\theta_{i0}^T
\end{align*}

\end{document}

