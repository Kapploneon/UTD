# Quality of Service

Realtime applications require __deliver on time__ assurances (muse come from
_inside_ the network)

How to achieve timely delivery?

- When one-way delay is small (< 1/3) relative to acceptable delay
    + Sent packets will arrive on timely
    + If lost, a packet has enough time to be retransmitted.
- When one-way delay is large (> 1) relative to acceptable delay
    + Impossible for retransmitted packets to arrive on time.
- Otherwise, packets _may_ arrive on time and __no possibility of
  retransmission__.

We can reduce the one-way delay by having faster links and/or prioritizing
traffic. However, one-way delay cannot be less than propagation delay.

- Within the 48-state U.S., one-way propagation-delay peeks around 25 msec.
- Humans notice about 50 msec delay for voice (not much chance for
  retransmission).

## Quality of Service Approaches

- Fine-grained

  Provide QoS to individual applications or flows. Proposed for the Internet:

    + IETF Integrated Services (IntServ) with Resource Reservation Protocol
      (RSVP, like a VC _setup_ )

    + You will need a __flow ID__, i.e., like virtual circuits

- Coarse-grained

  Provide QoS to large __classes__ of data or aggregated flows Proposed for the
  Internet:

    + IETF Differentiated Services (DiffServ)
    + You only need a few bits in the header to mark the __class__ of the packet

# Flow Specification

Flow Specification has two parts:

- __Rspec:__ describes ___service requested___ from network.

    + In IntServ’s guaranteed service: delay bound
    + In IntServ’s controlled-load service: none

- __Tspec:__ describes flow’s ___traffic___ characteristics

  Usually defined in terms of a rate _r_ and max burst size size _B_.

    + _r_ is the long-term rate of the flow
    + _B_ is the _burstiness_ or how much you can deviate from the rate r

## Constant Rate Service

Many QoS protocols guaranteed a constant rate _r_ to a network flow. Thus they
guarantee a packet will exit the network no later than the time it would exit
from a constant-rate server of rate _r_ (plus a small constant), i.e., the
network __mimics__ a constant-rate server as best it can.

So here's the question:

> For a given input flow _f_, what will be the delay through a constant rate
> server?

__The delay of a packet is the size of the queue when the packet arrives at the
server divided by the rate *r*.__

Thereby, we define the __(r,B) constrained flows__.

## (r,B) Constrained Flows

__A flow *f* is (r,B) constrained__ at some point P in the network __iff__
making a copy of f at P and giving it as input to a constant rate server of
rate _r_ causes __the server’s queue to grow to no more than B bytes__, i.e.,
its delay through a server of rate r is at most B/r

Note that a flow may be (r,B) constrained going into a router but may not be
when it leaves the router (due to delays at the router).

Given an input flow _f_ (i.e. if you know the arrival time of each packet of
_f_ and its size), can you determine if _f_ is (r,B) constrained?

> Sure, just compute the behavior of a constant rate server of rate _r_ and see
> how big its queue gets.

### Alternative (r,B) Constrained Flow Definition

A flow is (r,B) constrained iff, for any time and any interval of time of
length _t_, the number of bytes arriving from the flow are at most `r * t + B`.

### Proof for the Equivalence of Two Definitions

1. First part:

   If the queue of the server grows to no more than _B_, then the number of
   bytes arriving from the flow during any interval of length _t_ are at most
   `r * t+ B`.

   We can prove the contra-positive:

   > If the number of bytes arriving from the flow during some interval of
   > length _t_ are more than `r * t + B`, then the queue grows to more than
   > _B_.

   This is trivial, since in an interval of size _t_ we can transmit only `r *
   t` bytes, if more than `r * t + B` arrive, then the queue becomes greater
   than _B_.

2. Second part:

   If the number of bytes arriving from the flow at any time interval of size
   _t_ are at most `r * t + B`, then the queue of the server grows to no more
   than _B_.

   Prove again the contra-positive:

   > If the queue of the server grows to more than _B_, then there exists some
   > interval of size _t_ during which more than `r * t + B` bytes arrived.

   Let _x_ be the beginning of __busy period__ when the queue grew to more than
   _B_, and _y_ be the time when the queue grew to more than _B_, i.e., `queue
   = 0` right before _x_, `queue > 0` from _x_ to _y_,  `queue > B` at _y_.

   Since we are busy from _x_ to _y_, the server sent `(y - x) * r` bytes
   during [x,y].

   Since the queue was 0 before _x_, and more than _B_ at _y_, the total bytes
   that came in during [x,y] are more than `(y - x) * r + B`.

## Token Bucket

After define (r,B) constrained, now question becomes:

> Assume I don’t know _f_ in advance nor anything about the process that
> generates it. I want to ensure it is (r,B) constrained before giving it to
> the network (my tspec says that my traffic is (r,B) constrained). __How do I
> filter (smooth out) f so that it is (r,B) constrained?__

Note that the long-term average rate of _f_ has to be at most _r_, otherwise
you can’t, i.e., packets will be delayed in the filter forever.

Use ___token buckets___ (also known as leaky buckets).

With Token Bucket Capacity, _B_, and _r_ tokens/sec arrive continuously, each
byte needs a token in order to pass. If there is no token left when a packet
coming in, token bucket would, based on implementation,

- Dropping Filter: drops packets if token is not available.

- Buffered Filter: buffers data until tokens become available (we will assume
  buffered).

The _output channel_ rate of the bucket is infinity, i.e., tokens, and not
bandwidth, determine when the packet exits the bucket. We assume the bucket is
full initially 

__The output of a token bucket of parameters _r_ and _B_ is (r,B)
constrained.__

__Proof:__ Consider any interval of size _t_, there are at most _B_ token
initially, and `r * t` tokens would arrive during the interval. Hence, the
bucket cannot forward more than `B + r * t` during an interval of size _t_.

Hence, if the output of a token bucket with parameters _r,B_ is given to a
constant rate server of rate _r_, the queue in the server grows to at most _B_.

__If the input to a token bucket is (r,B) constrained then there is no delay in
the bucket.__

__Proof:__ Let _y_ be the first time when data arrive and there are not enough
tokens in the bucket for the data to leave, i.e., no data is queued at any time
before _y_. Let _x_ be the latest time such that _x_ < _y_ and the bucket is
full and during (_x_, _y_] the bucket is not full.

Initially, there are _B_ token at _x_, `(y - x) * r` tokens are generated
during the interval. No tokens are _lost_ by overflowing the bucket during
[_x_, _y_] since the bucket is not full during [_x_, _y_]. For _y_ to be
delayed, more than `B + r * (y - x)` packets must have been generated, result
in contradiction.

Hence, there should be no delay in the bucket.

# Weighted Fair Queuing (WFQ)

> Just like FQ...

Each flow _f_ is assigned a weight _Wf_. There is a fake server and a real
server. The bandwidth given to the flow in the fake server is proportional to
its weight,

![](http://latex.codecogs.com/gif.latex?\frac{W_f}{\sum_xW_x}\times{C}),

where C is the rate of the output channel, and the sum is over the set of
“backlogged flows” in the fake server (more on this later).

> FQ is WFQ with _w_ < 1 for all flow.

Real server forwards one packet at a time:

- It assigns timestamps to packets
- Packets are sent out in order of timestamp
- The timestamp is the “virtual” finishing time of the packet at the fake
  server

Bit-by-bit server forwards a few bits of each flow at a time (i.e. fractions of
a packet)

The virtual time V(t) at real time t is the _bit number_ or _round-number_ in
the fake bit-by-bit server at real time t. It is computed as follows

- Simple FQ: increase V(t) by 1 every time you forward one bit from ALL queued
  flows in the bit-by-bit server,
- WFQ: increase V(t) by 1 every time you forward _Wf_ bits from every queued
  flow _f_ in the bit-by-bit server.
- Thus, V(t) increases faster over time if there are less flows queued in the
  bit-by-bit server.

The _bit by bit round-robin service_ is therefore faster for flows with greater
weight.

![](http://latex.codecogs.com/gif.latex?F_{f,i}) (timestamp of _i_-th packet of
flow _f_) is the virtual (i.e. __fake__) time when the _i_-th packet of f exits
the fake server.

> ![](http://latex.codecogs.com/gif.latex?F_{f,i}=\max\(V\(A_{f,i}\),F\)+L_{f,i}/W_f)


Does the real scheduler forward packets bt smallest _D_ first?
