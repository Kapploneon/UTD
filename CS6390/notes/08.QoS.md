# Quality of Service

Realtime applications require __deliver on time__ assurances (muse come from
_inside_ the network)

How to achieve timely delivery?

- When one-way delay is small (< 1/3) relative to acceptable delay
    + Sent packets will arrive on timely
    + If lost, a packet has enough time to be retransmitted.
- When one-way delay is large (> 1) relative to acceptable delay
    + Impossible for retransmitted packets to arrive on time.
- Otherwise, packets _may_ arrive on time and __no possibility of
  retransmission__.

We can reduce the one-way delay by having faster links and/or prioritizing
traffic. However, one-way delay cannot be less than propagation delay.

- Within the 48-state U.S., one-way propagation-delay peeks around 25 msec.
- Humans notice about 50 msec delay for voice (not much chance for
  retransmission).

## Quality of Service Approaches

- Fine-grained

  Provide QoS to individual applications or flows. Proposed for the Internet:

    + IETF Integrated Services (IntServ) with Resource Reservation Protocol
      (RSVP, like a VC _setup_ )

    + You will need a __flow ID__, i.e., like virtual circuits

- Coarse-grained

  Provide QoS to large __classes__ of data or aggregated flows Proposed for the
  Internet:

    + IETF Differentiated Services (DiffServ)
    + You only need a few bits in the header to mark the __class__ of the packet

## A _Generic_ View of the Mechanisms

1. Source send __Reserve message__ to destination along the route, giving
   [_**flowspec**_](#flow-specification) (burstiness, rate, desired delay) and
   reserving resources along the way.

2. Routers along the way __Accept/Reject__  ([_**admission
   control**_](#admission-control)) according to available resources.

3. Source start sending data message, and router along the way do
   ___policing___, ensuring the source’s traffic satisfies the _flowspec_.

## Integrated Services - Mechanisms

- Flow specification: Tell the network the properties of the data flow

- Admission control: Network decides if it can handle flow

- Reservation: Reserve resources if flow admitted by admission control

- Packet classification: Map packets to flows

- Scheduling: Forwarding method

- Policing: Enforcing traffic properties at entrance of network

# Flow Specification

Flow Specification has two parts:

- __Rspec:__ describes ___service requested___ from network.

    + In IntServ’s guaranteed service: delay bound
    + In IntServ’s controlled-load service: none

- __Tspec:__ describes flow’s ___traffic___ characteristics

  Usually defined in terms of a rate _r_ and max burst size size _B_.

    + _r_ is the long-term rate of the flow
    + _B_ is the _burstiness_ or how much you can deviate from the rate r

## Constant Rate Service

Many QoS protocols guaranteed a constant rate _r_ to a network flow. Thus they
guarantee a packet will exit the network no later than the time it would exit
from a constant-rate server of rate _r_ (plus a small constant), i.e., the
network __mimics__ a constant-rate server as best it can.

So here's the question:

> For a given input flow _f_, what will be the delay through a constant rate
> server?

__The delay of a packet is the size of the queue when the packet arrives at the
server divided by the rate *r*.__

Thereby, we define the __(r,B) constrained flows__.

## (r,B) Constrained Flows

__A flow *f* is (r,B) constrained__ at some point P in the network __iff__
making a copy of f at P and giving it as input to a constant rate server of
rate _r_ causes __the server’s queue to grow to no more than B bytes__, i.e.,
its delay through a server of rate r is at most B/r

Note that a flow may be (r,B) constrained going into a router but may not be
when it leaves the router (due to delays at the router).

Given an input flow _f_ (i.e. if you know the arrival time of each packet of
_f_ and its size), can you determine if _f_ is (r,B) constrained?

> Sure, just compute the behavior of a constant rate server of rate _r_ and see
> how big its queue gets.

### Alternative (r,B) Constrained Flow Definition

A flow is (r,B) constrained iff, for any time and any interval of time of
length _t_, the number of bytes arriving from the flow are at most `r * t + B`.

### Proof for the Equivalence of Two Definitions

1. First part:

   If the queue of the server grows to no more than _B_, then the number of
   bytes arriving from the flow during any interval of length _t_ are at most
   `r * t+ B`.

   We can prove the contra-positive:

   > If the number of bytes arriving from the flow during some interval of
   > length _t_ are more than `r * t + B`, then the queue grows to more than
   > _B_.

   This is trivial, since in an interval of size _t_ we can transmit only `r *
   t` bytes, if more than `r * t + B` arrive, then the queue becomes greater
   than _B_.

2. Second part:

   If the number of bytes arriving from the flow at any time interval of size
   _t_ are at most `r * t + B`, then the queue of the server grows to no more
   than _B_.

   Prove again the contra-positive:

   > If the queue of the server grows to more than _B_, then there exists some
   > interval of size _t_ during which more than `r * t + B` bytes arrived.

   Let _x_ be the beginning of __busy period__ when the queue grew to more than
   _B_, and _y_ be the time when the queue grew to more than _B_, i.e., `queue
   = 0` right before _x_, `queue > 0` from _x_ to _y_,  `queue > B` at _y_.

   Since we are busy from _x_ to _y_, the server sent `(y - x) * r` bytes
   during [x,y].

   Since the queue was 0 before _x_, and more than _B_ at _y_, the total bytes
   that came in during [x,y] are more than `(y - x) * r + B`.

## Token Bucket

After define (r,B) constrained, now question becomes:

> Assume I don’t know _f_ in advance nor anything about the process that
> generates it. I want to ensure it is (r,B) constrained before giving it to
> the network (my tspec says that my traffic is (r,B) constrained). __How do I
> filter (smooth out) f so that it is (r,B) constrained?__

Note that the long-term average rate of _f_ has to be at most _r_, otherwise
you can’t, i.e., packets will be delayed in the filter forever.

Use ___token buckets___ (also known as leaky buckets).

With Token Bucket Capacity, _B_, and _r_ tokens/sec arrive continuously, each
byte needs a token in order to pass. If there is no token left when a packet
coming in, token bucket would, based on implementation,

- Dropping Filter: drops packets if token is not available.

- Buffered Filter: buffers data until tokens become available (we will assume
  buffered).

The _output channel_ rate of the bucket is infinity, i.e., tokens, and not
bandwidth, determine when the packet exits the bucket. We assume the bucket is
full initially 

__The output of a token bucket of parameters _r_ and _B_ is (r,B)
constrained.__

__Proof:__ Consider any interval of size _t_, there are at most _B_ token
initially, and `r * t` tokens would arrive during the interval. Hence, the
bucket cannot forward more than `B + r * t` during an interval of size _t_.

Hence, if the output of a token bucket with parameters _r,B_ is given to a
constant rate server of rate _r_, the queue in the server grows to at most _B_.

__If the input to a token bucket is (r,B) constrained then there is no delay in
the bucket.__

__Proof:__ Let _y_ be the first time when data arrive and there are not enough
tokens in the bucket for the data to leave, i.e., no data is queued at any time
before _y_. Let _x_ be the latest time such that _x_ < _y_ and the bucket is
full and during (_x_, _y_] the bucket is not full.

Initially, there are _B_ token at _x_, `(y - x) * r` tokens are generated
during the interval. No tokens are _lost_ by overflowing the bucket during
[_x_, _y_] since the bucket is not full during [_x_, _y_]. For _y_ to be
delayed, more than `B + r * (y - x)` packets must have been generated, result
in contradiction.

Hence, there should be no delay in the bucket.

# Per-Router Mechanisms

__Admission control__ decides if a new flow can be supported. The Answer depends on
service class and traffic specification. Admission control is not the same as
policing:

- policing shapes traffic at the entrance of the network to ensure it satisfies
  the _flowspec_ once the flow has been admitted.
- policing is usually done with a token bucket.

__Packet Processing__ include:

- Classification: associate each packet with the appropriate reservation, which
  is easy if we use flow id’s.
- Scheduling: manage queues so each packet receives the requested service.

## Virtual Clock Protocol

First some notation:

| Notation | Meaning |
| --- | --- |
| ![](http://latex.codecogs.com/svg.latex?C_{out}) | Capacity of the output channel |
| ![](http://latex.codecogs.com/svg.latex?L_{max}) | Maximum packet length allowed by multiplexor |
| ![](http://latex.codecogs.com/svg.latex?R_f) | Rate reserved by flow ![](http://latex.codecogs.com/svg.latex?f) |
| ![](http://latex.codecogs.com/svg.latex?p_f^i) | _i_-th packet of flow ![](http://latex.codecogs.com/svg.latex?f) |
| ![](http://latex.codecogs.com/svg.latex?L_f^i) | Length of packet ![](http://latex.codecogs.com/svg.latex?p_f^i) |
| ![](http://latex.codecogs.com/svg.latex?A_f^i) | Arrival time into multiplexor of the last bit of ![](http://latex.codecogs.com/svg.latex?p_f^i) |
| ![](http://latex.codecogs.com/svg.latex?E_f^i) | Exit time from multiplexor of the last bit of ![](http://latex.codecogs.com/svg.latex?p_f^i) |
| ![](http://latex.codecogs.com/svg.latex?T_f^i) | Timestamp assigned to ![](http://latex.codecogs.com/svg.latex?p_f^i) |
| ![](http://latex.codecogs.com/svg.latex?\Delta_f^i) | Delay of ![](http://latex.codecogs.com/svg.latex?p_f^i) at multiplexor, equal to ![](http://latex.codecogs.com/svg.latex?E_f^i-A_f^i) |

### Bounded Appetite Servers

Before presenting the Virtual Clock Multiplexors, we introduce Bounded Appetite
Servers (VC is a bounded appetite server).

Consider a server, where each packet
![](http://latex.codecogs.com/svg.latex?p_f^i) has a deadline
![](http://latex.codecogs.com/svg.latex?D_f^i). The server has ___bounded
appetite___ iff, for any interval of time
![](http://latex.codecogs.com/svg.latex?[t,t^\prime]), the total number of
bytes of packets arriving during the interval and whose deadline is at most
![](http://latex.codecogs.com/svg.latex?t^\prime) add to no more than
![](http://latex.codecogs.com/svg.latex?\(t^{\prime}-t\)}\times{C_{out}_{\phantom}).

That is:

<p align="center">
<!--<img src="https://goo.gl/whQB3C">-->
<img src="http://latex.codecogs.com/svg.latex?\left(\sum{i,j}:A_{f}^i\in[t,t^{\prime}]\land{D_{f}^i}\leq{t^{\prime}}:L_{f}^i\right\)\leq{\(t^{\prime}-t\)\times{C_{out}_{}}">
</p>

#### Lemma 1 (Bounded Appetite Exit Time)

Consider a server with bounded appetite. For every flow
![](http://latex.codecogs.com/svg.latex?f) and every
![](http://latex.codecogs.com/svg.latex?i),
![](http://latex.codecogs.com/svg.latex?i\geq{1}), then,
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?E_f^i\leq{D_f^i}+\frac{L_{max}}{C_{out}}">
</p>
provided the server is __work-conserving__, and it forwards packets in order of
deadline.

##### Proof

We assume  ![](http://latex.codecogs.com/svg.latex?{E_f^i}\geq{D_f^i}),
otherwise we have no proof obligation.

We would like to ﬁnd a time ![](http://latex.codecogs.com/svg.latex?t), where
![](http://latex.codecogs.com/svg.latex?t\leq{A_f^i}), such that:

1. During the interval ![](http://latex.codecogs.com/svg.latex?[t,E_f^i]), __the
   queue is never empty__.

   This rule above gives us a lower bound on the number of bytes sent before
   ![](http://latex.codecogs.com/svg.latex?p_f^i) exit.

2. During the interval ![](http://latex.codecogs.com/svg.latex?[t,E_f^i]),
   __only packets with deadlines at most__
   ![](http://latex.codecogs.com/svg.latex?{D_f^i}) __arriving after__
   ![](http://latex.codecogs.com/svg.latex?{t}) __are forwarded__.

   This rule gives us an upper bound on the number of bytes to be forwarded
   before ![](http://latex.codecogs.com/svg.latex?p_f^i) exit.

Consider packet ![](http://latex.codecogs.com/svg.latex?{p_f^i}). Let
![](http://latex.codecogs.com/svg.latex?{t}) be the ___latest___ time such that
![](http://latex.codecogs.com/svg.latex?t\leq{A_f^i}) and one of the following
holds:

1. The queue of the multiplexor is empty at time
   ![](http://latex.codecogs.com/svg.latex?{t}).

2. The multiplexor dequeues and forwards at time
   ![](http://latex.codecogs.com/svg.latex?{t}) a packet with deadline
   ![](http://latex.codecogs.com/svg.latex?{X}) such that
   ![](http://latex.codecogs.com/svg.latex?{X}>D_f^i)

From definition of ![](http://latex.codecogs.com/svg.latex?{t}) that it is the
latest time one of the condition hold, we know that neither condition 1 nor
condition 2 can hold during time interval
![](http://latex.codecogs.com/svg.latex?[{t},A_f^i]).

Then, we found ![](http://latex.codecogs.com/svg.latex?{t}) such that:

1. During the interval ![](http://latex.codecogs.com/svg.latex?[t,E_f^i]), the
   queue is never empty, since:

   + Queue is never empty in time interval
     ![](http://latex.codecogs.com/svg.latex?[{t},A_f^i]) by the definition (of
     __latest__).

   + Queue is never empty in time interval
     ![](http://latex.codecogs.com/svg.latex?[A_f^i,E_f^i]) since
     ![](http://latex.codecogs.com/svg.latex?p_f^i) is in the queue already.

2. During the interval ![](http://latex.codecogs.com/svg.latex?[t,E_f^i]), only
   packets with deadlines at most
   ![](http://latex.codecogs.com/svg.latex?{D_f^i}) arriving after
   ![](http://latex.codecogs.com/svg.latex?{t}) are forwarded, since:

   + At time ![](http://latex.codecogs.com/svg.latex?{t}), no packets are
     queued with deadline, by the definition (queue is either _empty_ or a
     message with greater deadline has been forwarded, which mean _any message
     with smaller deadline should be forwarded already_ at
     ![](http://latex.codecogs.com/svg.latex?{t})).

   + No packets with deadline greater than
     ![](http://latex.codecogs.com/svg.latex?{D_f^i}) can be forwarded, by the
     definition (of __latest__).

   + At time ![](http://latex.codecogs.com/svg.latex?t\leq{A_f^i}),
     ![](http://latex.codecogs.com/svg.latex?{p_f^i}) has already arrived in
     the system, thus no packet with deadline greater than
     ![](http://latex.codecogs.com/svg.latex?{D_f^i}) can be forwarded.

Therefore, we proved that:

- During time interval ![](http://latex.codecogs.com/svg.latex?[t,E_f^i]), only
  packets with deadline at most
  ![](http://latex.codecogs.com/svg.latex?{D_f^i}) that arrive during
  ![](http://latex.codecogs.com/svg.latex?[t,D_f^i]) are __dequeued and
  forwarded__.

- From the bounded appetite property, these bytes add to at most
  ![](http://latex.codecogs.com/svg.latex?\({D_f^i}-t\)\times{C_{out}_{\phantom}).

- The queue is always busy during time interval
  ![](http://latex.codecogs.com/svg.latex?[t,E_f^i]), so it takes at most
  ![](http://latex.codecogs.com/svg.latex?\({D_f^i}-t\)) seconds to forward
  these packets, so [](http://latex.codecogs.com/svg.latex?{p_f^i}) should exit
  by time ![](http://latex.codecogs.com/svg.latex?{D_f^i}).

- However, at time ![](http://latex.codecogs.com/svg.latex?{t}), a packet with
  deadline greater than ![](http://latex.codecogs.com/svg.latex?{D_f^i}) may be
  in the middle of being transmitted, and hence
  ![](http://latex.codecogs.com/svg.latex?E_f^i\leq{D_f^i}+\frac{L_{max}}{C_{out}}).

Proof ended. (Such a interesting proof!)
![](http://latex.codecogs.com/svg.latex?\square)

### Virtual Clock (VC) Multiplexors

Now go back to virtual clock (VC) protocol.

The ideal server would be like: each flow has its own capacity of the output
channel based on priority. In real life, we have only a single output channel
capacity for all the flow. A VC multiplexer makes the real server behave as
_good_ as the ideal servers. Exit time from real server should be at most the
exit time from ideal servers.

#### Method Overview

Each arriving packet is assigned a timestamp with the time the packet exits the
_ideal server_. Packets are maintained in a priority queue, whose priority is
the timestamp of the packet. When the channel becomes idle, the packet with
smallest timestamp is forwarded.

Let ![](http://latex.codecogs.com/svg.latex?{M}) be the number of packets in
the queue, then:

- Inserting a packet takes
  ![](http://latex.codecogs.com/svg.latex?\mathcal{O}\(\log{M}\))

- Removing a packet takes
  ![](http://latex.codecogs.com/svg.latex?\mathcal{O}\(\log{M}\))

Hence, VC have higher complexity than First-Come-First-Serve (FCFS), who has
![](http://latex.codecogs.com/svg.latex?\mathcal{O}\(1\)) overhead per packet.

The question is: _how to compute timestamp (priority)?_

- The ideal server of ![](http://latex.codecogs.com/svg.latex?f) serves a
  packet of size ![](http://latex.codecogs.com/svg.latex?L) in
  ![](http://latex.codecogs.com/svg.latex?\frac{L}{R_f}).

- Thus, the first packet exits the ideal server at time
  ![](http://latex.codecogs.com/svg.latex?A_f^1+\frac{L_f^1}{R_f})

- Packet ![](http://latex.codecogs.com/svg.latex?{p_f^i}) exit the ideal server
  when it begins service plus
  ![](http://latex.codecogs.com/svg.latex?\frac{L}{R_f}) second.

- And it will start after previous one or after itself arrival.

Thus, we have:

- ![](http://latex.codecogs.com/svg.latex?T_f^1=A_f^1+\frac{L_f^1}{R_f})

- ![](http://latex.codecogs.com/svg.latex?T_f^i=\max\left\(A_f^i,T_f^{i-1}\right\)+\frac{L_f^i}{R_f})

#### Main Theorem

__For every flow ![](http://latex.codecogs.com/svg.latex?{f}) and every
![](http://latex.codecogs.com/svg.latex?i,i\geq1),__
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?E_f^i\leq{T_f^i}+\frac{L_{max}}{C_{out}}">,
</p>

__provided,__

<p align="center">
<img src="http://latex.codecogs.com/svg.latex?\sum_{g=1}^N=R_g\leq{C_{out}}">
</p>

_Note that the above bound for flow ![](http://latex.codecogs.com/svg.latex?f)
is independent of any other flow._

#### Lemma 2 (VC has bounded appetite)

A VC server, where
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?\sum_{g=1}^N=R_g\leq{C_{out}}">
</p>
<p>
has bounded appetite, where <img src="http://latex.codecogs.com/svg.latex?{T_f^i=D_f^i}">.
</p>

That is, for any interval of time
![](http://latex.codecogs.com/svg.latex?[t,t^{\prime}]), the total number of
bytes of packets arriving during the interval and whose timestamp is at most 
![](http://latex.codecogs.com/svg.latex?t^{\prime}) add to no more than
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?\left\(t^{\prime}-t\right\)\times{C_{out}_{\phantom}}">
</p>

##### Proof
The proof for this lemma is intuitive.

Let ![](http://latex.codecogs.com/svg.latex?p_g^j) be the first packet of
![](http://latex.codecogs.com/svg.latex?g) after
![](http://latex.codecogs.com/svg.latex?t), and
![](http://latex.codecogs.com/svg.latex?p_g^n) be the last packet of
![](http://latex.codecogs.com/svg.latex?g) with
![](http://latex.codecogs.com/svg.latex?T_g^n\geq{t^{\prime}}). Then,
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?%5Cbegin%7Balign*%7DT_g%5Ej%5Cquad%26%5Cgeq%5Cquad%7Bt+%7B%5Cfrac%7BL_g%5Ej%7D%7BR_g%7D%7D%7D%5C%5CT_g%5E%7Bj+1%7D%5Cquad%26%5Cgeq%5Cquad%7BT_g%5Ej+%7B%5Cfrac%7BL_g%5E%7Bj+1%7D%7D%7BR_g%7D%7D%5Cgeq%7Bt+%7B%5Cfrac%7BL_g%5Ej%7D%7BR_g%7D%7D+%7B%5Cfrac%7BL_g%5E%7Bj+1%7D%7D%7BR_g%7D%7D%7D%7D%5C%5C%5Cldots%5Cquad%26%5C%5Ct%5E%7B%5Cprime%7D%5Cgeq%7BT_g%5En%7D%5Cquad%26%5Cgeq%5Cquad%7Bt+%5Csum_%7Bk%3Dj%7D%5En%7B%5Cfrac%7BL_g%5Ek%7D%7BR_g%7D%7D%7D%5Cend%7Balign*%7D">
</p>
Thus,
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?\sum_{k=j}^n{L_g^k\geq\(t^{\prime}-t\)\times{R_g}}">
</p>

Summing over all ![](http://latex.codecogs.com/svg.latex?g), the appetite of
all the flows during an interval
![](http://latex.codecogs.com/svg.latex?[t,t^{\prime}]) is:
<p align="center">
<img src="http://latex.codecogs.com/svg.latex?%5Csum_%7Bg%3D1%7D%5EN%7B%5C%28t%5E%7B%5Cprime%7D-t%5C%29%5Ctimes%7BR_g%7D%7D%3D%5C%28t%5E%7B%5Cprime%7D-t%5C%29%5Csum_%7Bg%3D1%7D%5EN%7BR_g%7D%5Cleq%5C%28t%5E%7B%5Cprime%7D-t%5C%29%5Ctimes%7BC_%7Bout%7D%7D">
</p>
Hence, VC is a bounded appetite
server.<img src="http://latex.codecogs.com/svg.latex?\square">


# Weighted Fair Queuing (WFQ)

> Just like FQ...

Each flow _f_ is assigned a weight _Wf_. There is a fake server and a real
server. The bandwidth given to the flow in the fake server is proportional to
its weight,

![](http://latex.codecogs.com/svg.latex?\frac{W_f}{\sum_{x}W_x}\times{C}),

where C is the rate of the output channel, and the sum is over the set of
“backlogged flows” in the fake server (more on this later).

> FQ is WFQ with _w_ < 1 for all flow.

Real server forwards one packet at a time:

- It assigns timestamps to packets
- Packets are sent out in order of timestamp
- The timestamp is the “virtual” finishing time of the packet at the fake
  server

Bit-by-bit server forwards a few bits of each flow at a time (i.e. fractions of
a packet)

The virtual time V(t) at real time t is the _bit number_ or _round-number_ in
the fake bit-by-bit server at real time t. It is computed as follows

- Simple FQ: increase V(t) by 1 every time you forward one bit from ALL queued
  flows in the bit-by-bit server,
- WFQ: increase V(t) by 1 every time you forward _Wf_ bits from every queued
  flow _f_ in the bit-by-bit server.
- Thus, V(t) increases faster over time if there are less flows queued in the
  bit-by-bit server.

The _bit by bit round-robin service_ is therefore faster for flows with greater
weight.

![](http://latex.codecogs.com/svg.latex?F_{f,i}) (timestamp of _i_-th packet of
flow _f_) is the virtual (i.e. __fake__) time when the _i_-th packet of f exits
the fake server.

> ![](http://latex.codecogs.com/svg.latex?F_{f,i}=\max\(V\(A_{f,i}\),F\)+L_{f,i}/W_f)


Does the real scheduler forward packets by smallest _D_ first?
